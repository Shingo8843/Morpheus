{"version":3,"file":"683.index.js","mappings":"qHACO,SAASA,EAAsBC,EAAQC,EAAOC,EAAY,EAAGC,GAChE,GAAc,IAAVF,EACA,MAAM,OAAY,uBAEtB,MAAMG,EAAiB,IAAIC,IACrBC,EAAwB,IAAID,IAC5BE,EAAaP,EAAOQ,OAC1B,IAAI,IAAIC,EAAI,EAAGA,EAAIF,EAAYE,IAAI,CAC/B,MAAMC,EAAMV,EAAOS,GACbE,EAAgBD,EAAIF,OAC1B,IAAI,IAAII,EAAI,EAAGA,EAAID,EAAeC,IAAI,CAClC,MAAOC,EAAOC,GAASJ,EAAIE,GACrBG,EAAaD,EAAQb,EACrBe,EAAWZ,EAAea,IAAIJ,QACnBK,IAAbF,GACAZ,EAAee,IAAIN,EAAkB,IAAXG,EAAiBD,GAC3CT,EAAsBa,IAAIN,EAAOP,EAAsBW,IAAIJ,GAAS,KAEpET,EAAee,IAAIN,EAAOE,GAC1BT,EAAsBa,IAAIN,EAAO,GAEzC,CACJ,CACA,MAAMO,EAAc,GACpB,IAAK,MAAMC,KAAmBjB,EAAekB,UACzCF,EAAYG,KAAKF,GAErB,MAAMG,EAAUJ,EAAYK,MAAK,CAACC,EAAGC,IAAIA,EAAE,GAAKD,EAAE,KAGlD,GAAkB,IAAdxB,EACA,OAAOsB,EAGX,MAAMI,EAAaJ,EAAQhB,OACrBqB,EAAqB,GAC3B,IAAK,MAAMC,KAA2BxB,EAAsBgB,UACxDO,EAAmBN,KAAKO,GAK5B,MAAMC,EAAmBF,EAAmBJ,MAAK,CAACC,EAAGC,IAAIA,EAAE,GAAKD,EAAE,KAClE,IAAIM,EACJ,IAAI,IAAIvB,EAAI,EAAGA,EAAImB,GACXG,EAAiBtB,GAAG,KAAON,EADJM,IAEvBuB,EAA2BvB,EAMnC,QAAwC,IAA7BuB,EAA0C,CACjD,GAAkB,IAAd9B,EACA,MAAO,GAEX8B,EAA2B,CAC/B,CAEA,GAAkB,IAAd9B,EACA,OAAOsB,EAAQS,MAAM,EAAGD,EAA2B,GAKvD,MAAME,EAAkBF,EAA2BG,KAAKC,KAAiB,IAAZlC,GAAmBsB,EAAQhB,OAASwB,GAA4B,KAC7H,OAAOR,EAAQS,MAAM,EAAGT,EAAQhB,OAAS0B,EAC7C,CACO,SAASG,EAAKC,EAAIC,EAAeC,EAAWC,EAAaC,EAAoBC,GAChF,MAAM,EAAEC,EAAE,EAAEjB,EAAE,EAAEkB,GAAOF,EAEvB,OADYR,KAAKW,IAAI,GAAKN,EAAYD,EAAgB,KAAQA,EAAgB,MAChEM,EAAIP,GAAMM,EAAI,KAAON,EAAKM,GAAK,EAAIjB,EAAIA,EAAIc,EAAcC,GAC3E,C,iBCtEI,SAASK,EAAoBrB,EAAGC,EAAGqB,GAEnC,GAAItB,IAAMC,EACN,OAAO,EAGX,MAAMsB,EAAOvB,EACTA,EAAElB,OAASmB,EAAEnB,SACbkB,EAAIC,EACJA,EAAIsB,GAER,IAAIC,EAAOxB,EAAElB,OACT2C,EAAOxB,EAAEnB,OAET4C,EAAW,EACf,KAAMA,EAAWF,GAAQxB,EAAE2B,WAAWD,KAAczB,EAAE0B,WAAWD,IAC7DA,IAKJ,GAAIA,IAAaF,EACb,OAAO,EAIX,KAAMA,EAAO,GAAKxB,EAAE2B,aAAaH,KAAUvB,EAAE0B,aAAaF,IACtDD,IACAC,IAGJ,IAAKD,EACD,OAAOC,EAAOH,GAAa,EAAIG,EAOnC,GALAD,GAAQE,EACRD,GAAQC,EAIJF,GAAQF,GAAaG,GAAQH,EAC7B,OAAOE,EAAOC,EAAOD,EAAOC,EAEhC,MAAMG,EAAQH,EAAOD,EACrB,GAAIF,EAAYG,EACZH,EAAYG,OACT,GAAIG,EAAQN,EACf,OAAQ,EAEZ,IAAIvC,EAAI,EACR,MAAM8C,EAAM,GACNC,EAAqB,GAC3B,KAAM/C,EAAIuC,GACNQ,EAAmB/C,GAAKkB,EAAE0B,WAAWD,EAAW3C,GAChD8C,EAAI9C,KAAOA,EAEf,KAAMA,EAAI0C,GACNK,EAAmB/C,GAAKkB,EAAE0B,WAAWD,EAAW3C,GAChD8C,EAAI9C,KAAOuC,EAAY,EAE3B,MAAMS,EAAST,EAAYM,EACrBI,EAAUV,EAAYG,EAC5B,IAAIQ,EAAS,EACTC,EAAOZ,EACPa,EAAU,EACVC,EAAO,EACPC,EAAQ,EACRC,EAAQ,EACRpD,EAAI,EAER,IAAIH,EAAI,EAAGA,EAAIyC,EAAMzC,IAAI,CAMrB,IALAqD,EAAOrD,EACPoD,EAAUpD,EAAI,EACduD,EAAQtC,EAAE2B,WAAWD,EAAW3C,GAChCkD,GAAUlD,EAAIgD,EAAS,EAAI,EAC3BG,GAAQA,EAAOT,EAAO,EAAI,EACtBvC,EAAI+C,EAAQ/C,EAAIgD,EAAMhD,IACtBmD,EAAQF,EACRA,EAAUC,EACVA,EAAOP,EAAI3C,GACPoD,IAAUR,EAAmB5C,KAEzBkD,EAAOD,IACPA,EAAUC,GAGVC,EAAQF,IACRA,EAAUE,GAEdF,KAEJN,EAAI3C,GAAKiD,EAEb,GAAIH,GAAWH,EAAI9C,EAAI6C,GAASN,EAC5B,OAAQ,CAEhB,CACA,OAAOa,GAAWb,EAAYa,GAAW,CAC7C,CAMWI,eAAeC,EAAmBxC,EAAGC,EAAGqB,GAC/C,MAAMmB,EAAWpB,EAAoBrB,EAAGC,EAAGqB,GAC3C,MAAO,CACHmB,WACAC,UAAWD,GAAY,EAE/B,CAEO,SAASE,EAAuB3C,EAAGC,EAAGqB,GACzC,MAAMmB,EAAWpB,EAAoBrB,EAAGC,EAAGqB,GAC3C,MAAO,CACHmB,WACAC,UAAWD,GAAY,EAE/B,C,sECxHO,MA+BMG,EAAY,CACrBC,MAAO,6BACPC,QAAS,6BACTC,OAAQ,iCACRC,QAAS,6BACTC,UAAW,6BACXC,WAAY,qBACZC,QAAS,wBACTC,QAAS,6BACTC,QAAS,0BACTC,OAAQ,0BACRC,QAAS,oBACTC,OAAQ,sBACRC,UAAW,kCACXC,SAAU,0BACVC,QAAS,0BACTC,QAAS,4BACTC,WAAY,kCACZC,OAAQ,mBACRC,OAAQ,mBACRC,MAAO,0BACPC,OAAQ,mBACRC,SAAU,mBACVC,MAAO,sBACPC,WAAY,gBACZC,UAAW,4BACXC,UAAW,sBACXC,UAAW,sBACXC,MAAO,mBACPC,SAAU,sCAEDC,EAAsBC,OAAOC,KA9DlB,CACpBd,OAAQ,KACRI,SAAU,KACVK,UAAW,KACXf,OAAQ,KACRX,MAAO,KACPC,QAAS,KACTS,QAAS,KACTR,OAAQ,KACRO,OAAQ,KACRa,MAAO,KACPV,UAAW,KACXQ,OAAQ,KACRG,WAAY,KACZJ,MAAO,KACPhB,QAAS,KACTa,WAAY,KACZE,OAAQ,KACRd,UAAW,KACXC,WAAY,KACZQ,SAAU,KACVP,QAAS,KACTQ,QAAS,KACTW,UAAW,KACXlB,QAAS,KACTC,QAAS,KACTmB,MAAO,KACPZ,QAAS,KACTS,UAAW,KACXI,SAAU,M,0DC3Bd,MACMI,EAAS,CACXC,kCAAmC,2EACnCC,uBAAwB,iEAHP,KAAoBC,KAAK,WAI1CC,8BAA+B,8CAC/BC,gBAAiB,mRACjBC,4CAA6C,qDAC7CC,sBAAuB,8BACvBC,2BAA4B,yCAC5BC,8CAA+C,kEAC/CC,oBAAqB,mGACrBC,2BAA4B,0DAC5BC,wBAAyB,0CACzBC,wBAAyB,2CACzBC,0BAA2B,oCAC3BC,0BAA2B,0DAC3BC,cAAe,uHACfC,oBAAqB,6DACrBC,yBAA0B,+DAC1BC,0BAA2B,yEAC3BC,yBAA0B,4EAC1BC,qBAAsB,8DACtBC,gCAAiC,2DACjCC,cAAe,oGACfC,0BAA2B,iCAC3BC,0BAA2B,sEAC3BC,wBAAyB,gCACzBC,oBAAqB,iEACrBC,qBAAsB,kEACtBC,qBAAsB,0PACtBC,2BAA4B,4EAC5BC,oBAAqB,wCACrBC,wBAAyB,2EACzBC,oBAAqB,6EACrBC,gCAAiC,kJACjCC,aAAc,8FACdC,qBAAsB,8GACtBC,eAAgB,gGAEb,SAASC,EAAYC,KAASC,GACjC,MAAMC,EAAQ,IAAIC,OAAM,QAAQ1C,EAAOuC,IAAS,iCAAiCA,OAAWC,IAK5F,OAJAC,EAAMF,KAAOA,EACT,sBAAuBG,MAAMC,WAC7BD,MAAME,kBAAkBH,GAErBA,CACX,C,6JC/CA,MAAMI,EAASC,KAAKC,MAAMC,WAAWtH,MAAM,GAC3C,IAAIuH,EAAS,EACb,MAAM5G,EAAI,KACJ6G,EAAOC,OAAO,KACdC,EAAQD,OAAO,KACfE,EAASF,OAAO,KAMLG,EAAyB,MAS/B,SAASC,EAAcpJ,EAAKqJ,GACnC,GAAIA,EAAOvJ,OAASqJ,EAChBG,MAAMd,UAAU3H,KAAK0I,MAAMvJ,EAAKqJ,QAEhC,IAAI,IAAItJ,EAAI,EAAGA,EAAIsJ,EAAOvJ,OAAQC,GAAKoJ,EACnCG,MAAMd,UAAU3H,KAAK0I,MAAMvJ,EAAKqJ,EAAO9H,MAAMxB,EAAGA,EAAIoJ,GAGhE,CACO,SAASK,EAAQC,KAAapB,GACjC,OAAOoB,EAASC,QAAQ,gEAAgE,YAAYC,GAChG,MAAMC,EAASD,EAAYA,EAAY7J,OAAS,IACxC+J,MAAOC,EAAS,KAAEC,EAAK,SAAEC,GAAcJ,EACzCK,EAAcD,EAAW3B,EAAK6B,OAAOC,SAASH,GAAY,GAAK3B,EAAK+B,QACpEP,EAAqB,KAAbC,EAAkB,EAAII,OAAOC,SAASL,GACpD,OAAOC,GACH,IAAK,IACD,OAAOE,EAAYpB,WAAWwB,SAASR,EAAO,KAClD,IAAK,IACD,CACI,IAAIS,EAAQL,EACZ,MAAOM,EAASC,GAAaV,EAASW,MAAM,KAAKC,KAAKC,GAAIT,OAAOU,WAAWD,KAI5E,MAHyB,iBAAdH,GAA0BA,GAAa,IAC9CF,EAAQA,EAAMO,QAAQL,IAEA,iBAAZD,GAAwBA,GAAW,EAAID,EAAMzB,WAAWwB,SAASR,EAAO,KAAOS,EAAMzB,UACvG,CACJ,IAAK,IACD,OAAOgB,EAAQ,EAAII,EAAYpB,WAAWiC,QAAQjB,EAAO,KAAOI,EAAYpB,WAAWwB,SAASR,EAAO,KAC3G,QACI,OAAOI,EAEnB,GACJ,CACO1G,eAAewH,EAAYC,EAAOC,EAAW,GAChD,GAAc,IAAVD,EACA,MAAO,UAEX,MAAME,EAAKD,EAAW,EAAI,EAAIA,EAYxBlL,EAAI0B,KAAK0J,MAAM1J,KAAKW,IAAI4I,GAASvJ,KAAKW,IAAIF,IAChD,MAAO,GAAG0I,YAAYI,EAAQvJ,KAAK2J,IAAIlJ,EAAGnC,IAAI8K,QAAQK,OAZxC,CACV,QACA,KACA,KACA,KACA,KACA,KACA,KACA,KACA,MAGgEnL,IACxE,CAQO,SAASsL,IACZ,OAAOrC,OAAOvH,KAAK0J,MAA0B,IAApBG,YAAY1C,OACzC,CACOrF,eAAegI,EAAkBjB,GAIpC,MAHqB,iBAAVA,IACPA,EAAQtB,OAAOsB,IAEfA,EAAQvB,EACD,GAAGuB,MACHA,EAAQrB,EACLqB,EAAQvB,EAAX,KACAuB,EAAQpB,EACLoB,EAAQrB,EAAX,KAEDqB,EAAQpB,EAAX,GACX,CACO3F,eAAeiI,IAClB,MAtBoC,oBAAtBC,mBAAqCC,gBAAgBD,kBAuBxDJ,IApBe,oBAAZM,SAA2BA,QAAQC,SAAoC,SAAzBD,QAAQC,QAAQC,MAyBrD,oBAAZF,cAA8CnL,IAAnBmL,QAAQG,OAFnCH,QAAQG,OAAOC,SAKC,oBAAhBT,YACAD,IAGJrC,OAAO,EAClB,CACOzF,eAAeyI,IAClB,MAAO,GAAGtD,KAAUI,KACxB,CACO,SAASmD,EAAeC,EAAQC,GAEnC,YAAsB3L,IAAlBmF,OAAOyG,OACAzG,OAAO6C,UAAU6D,eAAeC,KAAKJ,EAAQC,GAAYD,EAAOC,QAAY3L,EAEhFmF,OAAOyG,OAAOF,EAAQC,GAAYD,EAAOC,QAAY3L,CAChE,CAyBO,SAAS+L,EAAwBvL,EAAGC,GACvC,OAAIA,EAAE,KAAOD,EAAE,GACJA,EAAE,GAAKC,EAAE,GAEbA,EAAE,GAAKD,EAAE,EACpB,CAGO,SAASwL,EAAUlN,GACtB,GAAsB,IAAlBA,EAAOQ,OACP,MAAO,GACJ,GAAsB,IAAlBR,EAAOQ,OACd,OAAOR,EAAO,GAElB,IAAI,IAAIS,EAAI,EAAGA,EAAIT,EAAOQ,OAAQC,IAC9B,GAAIT,EAAOS,GAAGD,OAASR,EAAO,GAAGQ,OAAQ,CACrC,MAAM2M,EAAMnN,EAAO,GACnBA,EAAO,GAAKA,EAAOS,GACnBT,EAAOS,GAAK0M,CAChB,CAEJ,MAAMhM,EAAM,IAAId,IAChB,IAAK,MAAM+M,KAAQpN,EAAO,GACtBmB,EAAIA,IAAIiM,EAAM,GAElB,IAAI,IAAI3M,EAAI,EAAGA,EAAIT,EAAOQ,OAAQC,IAAI,CAClC,IAAI4M,EAAQ,EACZ,IAAK,MAAMD,KAAQpN,EAAOS,GAAG,CACzB,MAAM6M,EAAQnM,EAAIF,IAAImM,GAClBE,IAAU7M,IACVU,EAAIA,IAAIiM,EAAME,EAAQ,GACtBD,IAER,CACA,GAAc,IAAVA,EAAa,MAAO,EAC5B,CACA,OAAOrN,EAAO,GAAGuN,QAAQC,IACrB,MAAMF,EAAQnM,EAAIF,IAAIuM,GAEtB,YADctM,IAAVoM,GAAqBnM,EAAIA,IAAIqM,EAAG,GAC7BF,IAAUtN,EAAOQ,MAAM,GAEtC,CACOyD,eAAewJ,EAAsBC,EAAKC,GAC7C,MAAMC,EAAa,CAAC,EACdC,EAAcF,EAAMnN,OAC1B,IAAI,IAAIC,EAAI,EAAGA,EAAIoN,EAAapN,IAAI,CAChC,MAAMqN,EAAOH,EAAMlN,GACbsN,EAAaD,EAAK3C,MAAM,KAC9B,IAAItH,EAAU6J,EACd,MAAMM,EAAmBD,EAAWvN,OACpC,IAAI,IAAII,EAAI,EAAGA,EAAIoN,EAAkBpN,IAGjC,GAFAiD,EAAUA,EAAQkK,EAAWnN,IAEN,iBAAZiD,EAAsB,CAC7B,GAAgB,OAAZA,GAAoB,QAASA,GAAW,QAASA,GAAkC,iBAAhBA,EAAQoK,KAA2C,iBAAhBpK,EAAQqK,IAAkB,CAChIrK,EAAU+J,EAAWE,GAAQjK,EAC7B,KACJ,CAAO,IAAKmG,MAAMmE,QAAQtK,IAAwB,OAAZA,GAAoBjD,IAAMoN,EAAmB,EAAG,CAClFnK,OAAU3C,EACV,KACJ,CACJ,MAAO,IAAiB,OAAZ2C,GAAuC,iBAAZA,IAAyBjD,EAAIoN,EAAmB,EAAG,CAEtFnK,OAAU3C,EACV,KACJ,MAEmB,IAAZ2C,IACP+J,EAAWE,GAAQjK,EAE3B,CACA,OAAO+J,CACX,CACO3J,eAAemK,EAAUC,EAAKP,GAIjC,aAHoBL,EAAsBY,EAAK,CAC3CP,KAESA,EACjB,CAcA,MAAMQ,EAAsB,CACxBC,GAAI,IACJC,EAAG,EACHC,GAAI,IACJC,GAAI,MACJC,GAAI,MACJC,GAAI,UAED,SAASC,EAAwB1K,EAAU2K,GAC9C,MAAMC,EAAQT,EAAoBQ,GAClC,QAAc5N,IAAV6N,EACA,MAAM,IAAI9F,OAAM,OAAY,0BAA2B9E,GAAU6K,SAErE,OAAO7K,EAAW4K,CACtB,CACO,SAASE,EAAsBC,EAAcC,GAChDD,EAAaE,KAAOF,EAAaE,KAAKhE,KAAKiE,IAAS,IACzCA,EACHC,SAAU,IACHD,EAAOC,YAEPH,EAAiBI,QAAO,CAACC,EAAKC,KAC7B,MAAM3B,EAAO2B,EAAKtE,MAAM,KAClBuE,EAAU5B,EAAK6B,MACrB,IAAItB,EAAMmB,EACV,IAAK,MAAMI,KAAO9B,EACdO,EAAIuB,GAAOvB,EAAIuB,IAAQ,CAAC,EACxBvB,EAAMA,EAAIuB,GAGd,OADAvB,EAAIqB,GAAW,KACRF,CAAG,GACXH,EAAOC,cAG1B,C","sources":["webpack://morpheus/./node_modules/@orama/orama/dist/components/algorithms.js","webpack://morpheus/./node_modules/@orama/orama/dist/components/levenshtein.js","webpack://morpheus/./node_modules/@orama/orama/dist/components/tokenizer/languages.js","webpack://morpheus/./node_modules/@orama/orama/dist/errors.js","webpack://morpheus/./node_modules/@orama/orama/dist/utils.js"],"sourcesContent":["import { createError } from '../errors.js';\nexport function prioritizeTokenScores(arrays, boost, threshold = 1, keywordsCount) {\n    if (boost === 0) {\n        throw createError('INVALID_BOOST_VALUE');\n    }\n    const tokenScoresMap = new Map();\n    const tokenKeywordsCountMap = new Map();\n    const mapsLength = arrays.length;\n    for(let i = 0; i < mapsLength; i++){\n        const arr = arrays[i];\n        const entriesLength = arr.length;\n        for(let j = 0; j < entriesLength; j++){\n            const [token, score] = arr[j];\n            const boostScore = score * boost;\n            const oldScore = tokenScoresMap.get(token);\n            if (oldScore !== undefined) {\n                tokenScoresMap.set(token, oldScore * 1.5 + boostScore);\n                tokenKeywordsCountMap.set(token, tokenKeywordsCountMap.get(token) + 1);\n            } else {\n                tokenScoresMap.set(token, boostScore);\n                tokenKeywordsCountMap.set(token, 1);\n            }\n        }\n    }\n    const tokenScores = [];\n    for (const tokenScoreEntry of tokenScoresMap.entries()){\n        tokenScores.push(tokenScoreEntry);\n    }\n    const results = tokenScores.sort((a, b)=>b[1] - a[1]);\n    // If threshold is 1, it means we will return all the results with at least one search term,\n    // prioritizig the ones that contains more search terms (fuzzy match)\n    if (threshold === 1) {\n        return results;\n    }\n    // Prepare keywords count tracking for threshold handling\n    const allResults = results.length;\n    const tokenKeywordsCount = [];\n    for (const tokenKeywordsCountEntry of tokenKeywordsCountMap.entries()){\n        tokenKeywordsCount.push(tokenKeywordsCountEntry);\n    }\n    // Find the index of the last result with all keywords.\n    // Note that since score is multipled by 1.5 any time the token is encountered in results it means\n    // that tokenScores and tokenKeywordsCount should always have the same order.\n    const keywordsPerToken = tokenKeywordsCount.sort((a, b)=>b[1] - a[1]);\n    let lastTokenWithAllKeywords = undefined;\n    for(let i = 0; i < allResults; i++){\n        if (keywordsPerToken[i][1] === keywordsCount) {\n            lastTokenWithAllKeywords = i;\n        } else {\n            break;\n        }\n    }\n    // If no results had all the keywords, either bail out earlier or normalize\n    if (typeof lastTokenWithAllKeywords === 'undefined') {\n        if (threshold === 0) {\n            return [];\n        }\n        lastTokenWithAllKeywords = 0;\n    }\n    // If threshold is 0, it means we will only return all the results that contains ALL the search terms (exact match)\n    if (threshold === 0) {\n        return results.slice(0, lastTokenWithAllKeywords + 1);\n    }\n    // If the threshold is between 0 and 1, we will return all the results that contains at least the threshold of search terms\n    // For example, if threshold is 0.5, we will return all the results that contains at least 50% of the search terms\n    // (fuzzy match with a minimum threshold)\n    const thresholdLength = lastTokenWithAllKeywords + Math.ceil(threshold * 100 * (results.length - lastTokenWithAllKeywords) / 100);\n    return results.slice(0, results.length + thresholdLength);\n}\nexport function BM25(tf, matchingCount, docsCount, fieldLength, averageFieldLength, BM25Params) {\n    const { k , b , d  } = BM25Params;\n    const idf = Math.log(1 + (docsCount - matchingCount + 0.5) / (matchingCount + 0.5));\n    return idf * (d + tf * (k + 1)) / (tf + k * (1 - b + b * fieldLength / averageFieldLength));\n}\n\n//# sourceMappingURL=algorithms.js.map","/**\n * Inspired by:\n * https://github.com/Yomguithereal/talisman/blob/86ae55cbd040ff021d05e282e0e6c71f2dde21f8/src/metrics/levenshtein.js#L218-L340\n */ function _boundedLevenshtein(a, b, tolerance) {\n    // the strings are the same\n    if (a === b) {\n        return 0;\n    }\n    // a should be the shortest string\n    const swap = a;\n    if (a.length > b.length) {\n        a = b;\n        b = swap;\n    }\n    let lenA = a.length;\n    let lenB = b.length;\n    // ignore common prefix\n    let startIdx = 0;\n    while(startIdx < lenA && a.charCodeAt(startIdx) === b.charCodeAt(startIdx)){\n        startIdx++;\n    }\n    // if string A is subfix of B, we consider the distance 0\n    // because we search for prefix!\n    // fix https://github.com/oramasearch/orama/issues/544\n    if (startIdx === lenA) {\n        return 0;\n    }\n    // ignore common suffix\n    // note: `~-` decreases by a unit in a bitwise fashion\n    while(lenA > 0 && a.charCodeAt(~-lenA) === b.charCodeAt(~-lenB)){\n        lenA--;\n        lenB--;\n    }\n    // early return when the smallest string is empty\n    if (!lenA) {\n        return lenB > tolerance ? -1 : lenB;\n    }\n    lenA -= startIdx;\n    lenB -= startIdx;\n    // If both strings are smaller than the tolerance, we accept any distance\n    // Probably the result distance is wrong, but we don't care:\n    // It is always less then the tolerance!\n    if (lenA <= tolerance && lenB <= tolerance) {\n        return lenA > lenB ? lenA : lenB;\n    }\n    const delta = lenB - lenA;\n    if (tolerance > lenB) {\n        tolerance = lenB;\n    } else if (delta > tolerance) {\n        return -1;\n    }\n    let i = 0;\n    const row = [];\n    const characterCodeCache = [];\n    while(i < tolerance){\n        characterCodeCache[i] = b.charCodeAt(startIdx + i);\n        row[i] = ++i;\n    }\n    while(i < lenB){\n        characterCodeCache[i] = b.charCodeAt(startIdx + i);\n        row[i++] = tolerance + 1;\n    }\n    const offset = tolerance - delta;\n    const haveMax = tolerance < lenB;\n    let jStart = 0;\n    let jEnd = tolerance;\n    let current = 0;\n    let left = 0;\n    let above = 0;\n    let charA = 0;\n    let j = 0;\n    // Starting the nested loops\n    for(i = 0; i < lenA; i++){\n        left = i;\n        current = i + 1;\n        charA = a.charCodeAt(startIdx + i);\n        jStart += i > offset ? 1 : 0;\n        jEnd += jEnd < lenB ? 1 : 0;\n        for(j = jStart; j < jEnd; j++){\n            above = current;\n            current = left;\n            left = row[j];\n            if (charA !== characterCodeCache[j]) {\n                // insert current\n                if (left < current) {\n                    current = left;\n                }\n                // delete current\n                if (above < current) {\n                    current = above;\n                }\n                current++;\n            }\n            row[j] = current;\n        }\n        if (haveMax && row[i + delta] > tolerance) {\n            return -1;\n        }\n    }\n    return current <= tolerance ? current : -1;\n}\n/**\n * Computes the Levenshtein distance between two strings (a, b), returning early with -1 if the distance\n * is greater than the given tolerance.\n * It assumes that:\n * - tolerance >= ||a| - |b|| >= 0\n */ export async function boundedLevenshtein(a, b, tolerance) {\n    const distance = _boundedLevenshtein(a, b, tolerance);\n    return {\n        distance,\n        isBounded: distance >= 0\n    };\n}\n// This is only used internally, keep in sync with the previous one\nexport function syncBoundedLevenshtein(a, b, tolerance) {\n    const distance = _boundedLevenshtein(a, b, tolerance);\n    return {\n        distance,\n        isBounded: distance >= 0\n    };\n}\nexport function levenshtein(a, b) {\n    /* c8 ignore next 3 */ if (!a.length) {\n        return b.length;\n    }\n    /* c8 ignore next 3 */ if (!b.length) {\n        return a.length;\n    }\n    const swap = a;\n    if (a.length > b.length) {\n        a = b;\n        b = swap;\n    }\n    const row = Array.from({\n        length: a.length + 1\n    }, (_, i)=>i);\n    let val = 0;\n    for(let i = 1; i <= b.length; i++){\n        let prev = i;\n        for(let j = 1; j <= a.length; j++){\n            if (b[i - 1] === a[j - 1]) {\n                val = row[j - 1];\n            } else {\n                val = Math.min(row[j - 1] + 1, Math.min(prev + 1, row[j] + 1));\n            }\n            row[j - 1] = prev;\n            prev = val;\n        }\n        row[a.length] = prev;\n    }\n    return row[a.length];\n}\n\n//# sourceMappingURL=levenshtein.js.map","export const STEMMERS = {\n    arabic: 'ar',\n    armenian: 'am',\n    bulgarian: 'bg',\n    danish: 'dk',\n    dutch: 'nl',\n    english: 'en',\n    finnish: 'fi',\n    french: 'fr',\n    german: 'de',\n    greek: 'gr',\n    hungarian: 'hu',\n    indian: 'in',\n    indonesian: 'id',\n    irish: 'ie',\n    italian: 'it',\n    lithuanian: 'lt',\n    nepali: 'np',\n    norwegian: 'no',\n    portuguese: 'pt',\n    romanian: 'ro',\n    russian: 'ru',\n    serbian: 'rs',\n    slovenian: 'ru',\n    spanish: 'es',\n    swedish: 'se',\n    tamil: 'ta',\n    turkish: 'tr',\n    ukrainian: 'uk',\n    sanskrit: 'sk'\n};\nexport const SPLITTERS = {\n    dutch: /[^A-Za-zàèéìòóù0-9_'-]+/gim,\n    english: /[^A-Za-zàèéìòóù0-9_'-]+/gim,\n    french: /[^a-z0-9äâàéèëêïîöôùüûœç-]+/gim,\n    italian: /[^A-Za-zàèéìòóù0-9_'-]+/gim,\n    norwegian: /[^a-z0-9_æøåÆØÅäÄöÖüÜ]+/gim,\n    portuguese: /[^a-z0-9à-úÀ-Ú]/gim,\n    russian: /[^a-z0-9а-яА-ЯёЁ]+/gim,\n    spanish: /[^a-z0-9A-Zá-úÁ-ÚñÑüÜ]+/gim,\n    swedish: /[^a-z0-9_åÅäÄöÖüÜ-]+/gim,\n    german: /[^a-z0-9A-ZäöüÄÖÜß]+/gim,\n    finnish: /[^a-z0-9äöÄÖ]+/gim,\n    danish: /[^a-z0-9æøåÆØÅ]+/gim,\n    hungarian: /[^a-z0-9áéíóöőúüűÁÉÍÓÖŐÚÜŰ]+/gim,\n    romanian: /[^a-z0-9ăâîșțĂÂÎȘȚ]+/gim,\n    serbian: /[^a-z0-9čćžšđČĆŽŠĐ]+/gim,\n    turkish: /[^a-z0-9çÇğĞıİöÖşŞüÜ]+/gim,\n    lithuanian: /[^a-z0-9ąčęėįšųūžĄČĘĖĮŠŲŪŽ]+/gim,\n    arabic: /[^a-z0-9أ-ي]+/gim,\n    nepali: /[^a-z0-9अ-ह]+/gim,\n    irish: /[^a-z0-9áéíóúÁÉÍÓÚ]+/gim,\n    indian: /[^a-z0-9अ-ह]+/gim,\n    armenian: /[^a-z0-9ա-ֆ]+/gim,\n    greek: /[^a-z0-9α-ωά-ώ]+/gim,\n    indonesian: /[^a-z0-9]+/gim,\n    ukrainian: /[^a-z0-9а-яА-ЯіїєІЇЄ]+/gim,\n    slovenian: /[^a-z0-9čžšČŽŠ]+/gim,\n    bulgarian: /[^a-z0-9а-яА-Я]+/gim,\n    tamil: /[^a-z0-9அ-ஹ]+/gim,\n    sanskrit: /[^a-z0-9A-Zāīūṛḷṃṁḥśṣṭḍṇṅñḻḹṝ]+/gim\n};\nexport const SUPPORTED_LANGUAGES = Object.keys(STEMMERS);\n\n//# sourceMappingURL=languages.js.map","import { SUPPORTED_LANGUAGES } from './components/tokenizer/languages.js';\nimport { sprintf } from './utils.js';\nconst allLanguages = SUPPORTED_LANGUAGES.join('\\n - ');\nconst errors = {\n    NO_LANGUAGE_WITH_CUSTOM_TOKENIZER: 'Do not pass the language option to create when using a custom tokenizer.',\n    LANGUAGE_NOT_SUPPORTED: `Language \"%s\" is not supported.\\nSupported languages are:\\n - ${allLanguages}`,\n    INVALID_STEMMER_FUNCTION_TYPE: `config.stemmer property must be a function.`,\n    MISSING_STEMMER: `As of version 1.0.0 @orama/orama does not ship non English stemmers by default. To solve this, please explicitly import and specify the \"%s\" stemmer from the package @orama/stemmers. See https://docs.oramasearch.com/open-source/text-analysis/stemming for more information.`,\n    CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY: 'Custom stop words array must only contain strings.',\n    UNSUPPORTED_COMPONENT: `Unsupported component \"%s\".`,\n    COMPONENT_MUST_BE_FUNCTION: `The component \"%s\" must be a function.`,\n    COMPONENT_MUST_BE_FUNCTION_OR_ARRAY_FUNCTIONS: `The component \"%s\" must be a function or an array of functions.`,\n    INVALID_SCHEMA_TYPE: `Unsupported schema type \"%s\" at \"%s\". Expected \"string\", \"boolean\" or \"number\" or array of them.`,\n    DOCUMENT_ID_MUST_BE_STRING: `Document id must be of type \"string\". Got \"%s\" instead.`,\n    DOCUMENT_ALREADY_EXISTS: `A document with id \"%s\" already exists.`,\n    DOCUMENT_DOES_NOT_EXIST: `A document with id \"%s\" does not exists.`,\n    MISSING_DOCUMENT_PROPERTY: `Missing searchable property \"%s\".`,\n    INVALID_DOCUMENT_PROPERTY: `Invalid document property \"%s\": expected \"%s\", got \"%s\"`,\n    UNKNOWN_INDEX: `Invalid property name \"%s\". Expected a wildcard string (\"*\") or array containing one of the following properties: %s`,\n    INVALID_BOOST_VALUE: `Boost value must be a number greater than, or less than 0.`,\n    INVALID_FILTER_OPERATION: `You can only use one operation per filter, you requested %d.`,\n    SCHEMA_VALIDATION_FAILURE: `Cannot insert document due schema validation failure on \"%s\" property.`,\n    INVALID_SORT_SCHEMA_TYPE: `Unsupported sort schema type \"%s\" at \"%s\". Expected \"string\" or \"number\".`,\n    CANNOT_SORT_BY_ARRAY: `Cannot configure sort for \"%s\" because it is an array (%s).`,\n    UNABLE_TO_SORT_ON_UNKNOWN_FIELD: `Unable to sort on unknown field \"%s\". Allowed fields: %s`,\n    SORT_DISABLED: `Sort is disabled. Please read the documentation at https://docs.oramasearch for more information.`,\n    UNKNOWN_GROUP_BY_PROPERTY: `Unknown groupBy property \"%s\".`,\n    INVALID_GROUP_BY_PROPERTY: `Invalid groupBy property \"%s\". Allowed types: \"%s\", but given \"%s\".`,\n    UNKNOWN_FILTER_PROPERTY: `Unknown filter property \"%s\".`,\n    INVALID_VECTOR_SIZE: `Vector size must be a number greater than 0. Got \"%s\" instead.`,\n    INVALID_VECTOR_VALUE: `Vector value must be a number greater than 0. Got \"%s\" instead.`,\n    INVALID_INPUT_VECTOR: `Property \"%s\" was declared as a %s-dimensional vector, but got a %s-dimensional vector instead.\\nInput vectors must be of the size declared in the schema, as calculating similarity between vectors of different sizes can lead to unexpected results.`,\n    WRONG_SEARCH_PROPERTY_TYPE: `Property \"%s\" is not searchable. Only \"string\" properties are searchable.`,\n    FACET_NOT_SUPPORTED: `Facet doens't support the type \"%s\".`,\n    INVALID_DISTANCE_SUFFIX: `Invalid distance suffix \"%s\". Valid suffixes are: cm, m, km, mi, yd, ft.`,\n    INVALID_SEARCH_MODE: `Invalid search mode \"%s\". Valid modes are: \"fulltext\", \"vector\", \"hybrid\".`,\n    MISSING_VECTOR_AND_SECURE_PROXY: `No vector was provided and no secure proxy was configured. Please provide a vector or configure an Orama Secure Proxy to perform hybrid search.`,\n    MISSING_TERM: `\"term\" is a required parameter when performing hybrid search. Please provide a search term.`,\n    INVALID_VECTOR_INPUT: `Invalid \"vector\" property. Expected an object with \"value\" and \"property\" properties, but got \"%s\" instead.`,\n    PLUGIN_CRASHED: `A plugin crashed during initialization. Please check the error message for more information:`\n};\nexport function createError(code, ...args) {\n    const error = new Error(sprintf(errors[code] ?? `Unsupported Orama Error code: ${code}`, ...args));\n    error.code = code;\n    if ('captureStackTrace' in Error.prototype) {\n        Error.captureStackTrace(error);\n    }\n    return error;\n}\n\n//# sourceMappingURL=errors.js.map","import { createError } from './errors.js';\nconst baseId = Date.now().toString().slice(5);\nlet lastId = 0;\nconst k = 1024;\nconst nano = BigInt(1e3);\nconst milli = BigInt(1e6);\nconst second = BigInt(1e9);\nexport const isServer = typeof window === 'undefined';\n/**\n * This value can be increased up to 100_000\n * But i don't know if this value change from nodejs to nodejs\n * So I will keep a safer value here.\n */ export const MAX_ARGUMENT_FOR_STACK = 65535;\n/**\n * This method is needed to used because of issues like: https://github.com/oramasearch/orama/issues/301\n * that issue is caused because the array that is pushed is huge (>100k)\n *\n * @example\n * ```ts\n * safeArrayPush(myArray, [1, 2])\n * ```\n */ export function safeArrayPush(arr, newArr) {\n    if (newArr.length < MAX_ARGUMENT_FOR_STACK) {\n        Array.prototype.push.apply(arr, newArr);\n    } else {\n        for(let i = 0; i < newArr.length; i += MAX_ARGUMENT_FOR_STACK){\n            Array.prototype.push.apply(arr, newArr.slice(i, i + MAX_ARGUMENT_FOR_STACK));\n        }\n    }\n}\nexport function sprintf(template, ...args) {\n    return template.replace(/%(?:(?<position>\\d+)\\$)?(?<width>-?\\d*\\.?\\d*)(?<type>[dfs])/g, function(...replaceArgs) {\n        const groups = replaceArgs[replaceArgs.length - 1];\n        const { width: rawWidth , type , position  } = groups;\n        const replacement = position ? args[Number.parseInt(position) - 1] : args.shift();\n        const width = rawWidth === '' ? 0 : Number.parseInt(rawWidth);\n        switch(type){\n            case 'd':\n                return replacement.toString().padStart(width, '0');\n            case 'f':\n                {\n                    let value = replacement;\n                    const [padding, precision] = rawWidth.split('.').map((w)=>Number.parseFloat(w));\n                    if (typeof precision === 'number' && precision >= 0) {\n                        value = value.toFixed(precision);\n                    }\n                    return typeof padding === 'number' && padding >= 0 ? value.toString().padStart(width, '0') : value.toString();\n                }\n            case 's':\n                return width < 0 ? replacement.toString().padEnd(-width, ' ') : replacement.toString().padStart(width, ' ');\n            default:\n                return replacement;\n        }\n    });\n}\nexport async function formatBytes(bytes, decimals = 2) {\n    if (bytes === 0) {\n        return '0 Bytes';\n    }\n    const dm = decimals < 0 ? 0 : decimals;\n    const sizes = [\n        'Bytes',\n        'KB',\n        'MB',\n        'GB',\n        'TB',\n        'PB',\n        'EB',\n        'ZB',\n        'YB'\n    ];\n    const i = Math.floor(Math.log(bytes) / Math.log(k));\n    return `${parseFloat((bytes / Math.pow(k, i)).toFixed(dm))} ${sizes[i]}`;\n}\nexport function isInsideWebWorker() {\n    // @ts-expect-error - WebWorker global scope\n    return typeof WorkerGlobalScope !== 'undefined' && self instanceof WorkerGlobalScope;\n}\nexport function isInsideNode() {\n    return typeof process !== 'undefined' && process.release && process.release.name === 'node';\n}\nexport function getNanosecondTimeViaPerformance() {\n    return BigInt(Math.floor(performance.now() * 1e6));\n}\nexport async function formatNanoseconds(value) {\n    if (typeof value === 'number') {\n        value = BigInt(value);\n    }\n    if (value < nano) {\n        return `${value}ns`;\n    } else if (value < milli) {\n        return `${value / nano}μs`;\n    } else if (value < second) {\n        return `${value / milli}ms`;\n    }\n    return `${value / second}s`;\n}\nexport async function getNanosecondsTime() {\n    if (isInsideWebWorker()) {\n        return getNanosecondTimeViaPerformance();\n    }\n    if (isInsideNode()) {\n        return process.hrtime.bigint();\n    }\n    if (typeof process !== 'undefined' && process.hrtime !== undefined) {\n        return process.hrtime.bigint();\n    }\n    if (typeof performance !== 'undefined') {\n        return getNanosecondTimeViaPerformance();\n    }\n    // @todo: fallback to V8 native method to get microtime\n    return BigInt(0);\n}\nexport async function uniqueId() {\n    return `${baseId}-${lastId++}`;\n}\nexport function getOwnProperty(object, property) {\n    // Checks if `hasOwn` method is defined avoiding errors with older Node.js versions\n    if (Object.hasOwn === undefined) {\n        return Object.prototype.hasOwnProperty.call(object, property) ? object[property] : undefined;\n    }\n    return Object.hasOwn(object, property) ? object[property] : undefined;\n}\nexport function getTokenFrequency(token, tokens) {\n    let count = 0;\n    for (const t of tokens){\n        if (t === token) {\n            count++;\n        }\n    }\n    return count;\n}\nexport function insertSortedValue(arr, el, compareFn = sortTokenScorePredicate) {\n    let low = 0;\n    let high = arr.length;\n    let mid;\n    while(low < high){\n        mid = low + high >>> 1;\n        if (compareFn(el, arr[mid]) < 0) {\n            high = mid;\n        } else {\n            low = mid + 1;\n        }\n    }\n    arr.splice(low, 0, el);\n    return arr;\n}\nexport function sortTokenScorePredicate(a, b) {\n    if (b[1] === a[1]) {\n        return a[0] - b[0];\n    }\n    return b[1] - a[1];\n}\n// Intersection function taken from https://github.com/lovasoa/fast_array_intersect.\n// MIT Licensed at the time of writing.\nexport function intersect(arrays) {\n    if (arrays.length === 0) {\n        return [];\n    } else if (arrays.length === 1) {\n        return arrays[0];\n    }\n    for(let i = 1; i < arrays.length; i++){\n        if (arrays[i].length < arrays[0].length) {\n            const tmp = arrays[0];\n            arrays[0] = arrays[i];\n            arrays[i] = tmp;\n        }\n    }\n    const set = new Map();\n    for (const elem of arrays[0]){\n        set.set(elem, 1);\n    }\n    for(let i = 1; i < arrays.length; i++){\n        let found = 0;\n        for (const elem of arrays[i]){\n            const count = set.get(elem);\n            if (count === i) {\n                set.set(elem, count + 1);\n                found++;\n            }\n        }\n        if (found === 0) return [];\n    }\n    return arrays[0].filter((e)=>{\n        const count = set.get(e);\n        if (count !== undefined) set.set(e, 0);\n        return count === arrays.length;\n    });\n}\nexport async function getDocumentProperties(doc, paths) {\n    const properties = {};\n    const pathsLength = paths.length;\n    for(let i = 0; i < pathsLength; i++){\n        const path = paths[i];\n        const pathTokens = path.split('.');\n        let current = doc;\n        const pathTokensLength = pathTokens.length;\n        for(let j = 0; j < pathTokensLength; j++){\n            current = current[pathTokens[j]];\n            // We found an object but we were supposed to be done\n            if (typeof current === 'object') {\n                if (current !== null && 'lat' in current && 'lon' in current && typeof current.lat === 'number' && typeof current.lon === 'number') {\n                    current = properties[path] = current;\n                    break;\n                } else if (!Array.isArray(current) && current !== null && j === pathTokensLength - 1) {\n                    current = undefined;\n                    break;\n                }\n            } else if ((current === null || typeof current !== 'object') && j < pathTokensLength - 1) {\n                // We can't recurse anymore but we were supposed to\n                current = undefined;\n                break;\n            }\n        }\n        if (typeof current !== 'undefined') {\n            properties[path] = current;\n        }\n    }\n    return properties;\n}\nexport async function getNested(obj, path) {\n    const props = await getDocumentProperties(obj, [\n        path\n    ]);\n    return props[path];\n}\nexport function flattenObject(obj, prefix = '') {\n    const result = {};\n    for(const key in obj){\n        const prop = `${prefix}${key}`;\n        const objKey = obj[key];\n        if (typeof objKey === 'object' && objKey !== null) {\n            Object.assign(result, flattenObject(objKey, `${prop}.`));\n        } else {\n            result[prop] = objKey;\n        }\n    }\n    return result;\n}\nconst mapDistanceToMeters = {\n    cm: 0.01,\n    m: 1,\n    km: 1000,\n    ft: 0.3048,\n    yd: 0.9144,\n    mi: 1609.344\n};\nexport function convertDistanceToMeters(distance, unit) {\n    const ratio = mapDistanceToMeters[unit];\n    if (ratio === undefined) {\n        throw new Error(createError('INVALID_DISTANCE_SUFFIX', distance).message);\n    }\n    return distance * ratio;\n}\nexport function removeVectorsFromHits(searchResult, vectorProperties) {\n    searchResult.hits = searchResult.hits.map((result)=>({\n            ...result,\n            document: {\n                ...result.document,\n                // Remove embeddings from the result\n                ...vectorProperties.reduce((acc, prop)=>{\n                    const path = prop.split('.');\n                    const lastKey = path.pop();\n                    let obj = acc;\n                    for (const key of path){\n                        obj[key] = obj[key] ?? {};\n                        obj = obj[key];\n                    }\n                    obj[lastKey] = null;\n                    return acc;\n                }, result.document)\n            }\n        }));\n}\n\n//# sourceMappingURL=utils.js.map"],"names":["prioritizeTokenScores","arrays","boost","threshold","keywordsCount","tokenScoresMap","Map","tokenKeywordsCountMap","mapsLength","length","i","arr","entriesLength","j","token","score","boostScore","oldScore","get","undefined","set","tokenScores","tokenScoreEntry","entries","push","results","sort","a","b","allResults","tokenKeywordsCount","tokenKeywordsCountEntry","keywordsPerToken","lastTokenWithAllKeywords","slice","thresholdLength","Math","ceil","BM25","tf","matchingCount","docsCount","fieldLength","averageFieldLength","BM25Params","k","d","log","_boundedLevenshtein","tolerance","swap","lenA","lenB","startIdx","charCodeAt","delta","row","characterCodeCache","offset","haveMax","jStart","jEnd","current","left","above","charA","async","boundedLevenshtein","distance","isBounded","syncBoundedLevenshtein","SPLITTERS","dutch","english","french","italian","norwegian","portuguese","russian","spanish","swedish","german","finnish","danish","hungarian","romanian","serbian","turkish","lithuanian","arabic","nepali","irish","indian","armenian","greek","indonesian","ukrainian","slovenian","bulgarian","tamil","sanskrit","SUPPORTED_LANGUAGES","Object","keys","errors","NO_LANGUAGE_WITH_CUSTOM_TOKENIZER","LANGUAGE_NOT_SUPPORTED","join","INVALID_STEMMER_FUNCTION_TYPE","MISSING_STEMMER","CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY","UNSUPPORTED_COMPONENT","COMPONENT_MUST_BE_FUNCTION","COMPONENT_MUST_BE_FUNCTION_OR_ARRAY_FUNCTIONS","INVALID_SCHEMA_TYPE","DOCUMENT_ID_MUST_BE_STRING","DOCUMENT_ALREADY_EXISTS","DOCUMENT_DOES_NOT_EXIST","MISSING_DOCUMENT_PROPERTY","INVALID_DOCUMENT_PROPERTY","UNKNOWN_INDEX","INVALID_BOOST_VALUE","INVALID_FILTER_OPERATION","SCHEMA_VALIDATION_FAILURE","INVALID_SORT_SCHEMA_TYPE","CANNOT_SORT_BY_ARRAY","UNABLE_TO_SORT_ON_UNKNOWN_FIELD","SORT_DISABLED","UNKNOWN_GROUP_BY_PROPERTY","INVALID_GROUP_BY_PROPERTY","UNKNOWN_FILTER_PROPERTY","INVALID_VECTOR_SIZE","INVALID_VECTOR_VALUE","INVALID_INPUT_VECTOR","WRONG_SEARCH_PROPERTY_TYPE","FACET_NOT_SUPPORTED","INVALID_DISTANCE_SUFFIX","INVALID_SEARCH_MODE","MISSING_VECTOR_AND_SECURE_PROXY","MISSING_TERM","INVALID_VECTOR_INPUT","PLUGIN_CRASHED","createError","code","args","error","Error","prototype","captureStackTrace","baseId","Date","now","toString","lastId","nano","BigInt","milli","second","MAX_ARGUMENT_FOR_STACK","safeArrayPush","newArr","Array","apply","sprintf","template","replace","replaceArgs","groups","width","rawWidth","type","position","replacement","Number","parseInt","shift","padStart","value","padding","precision","split","map","w","parseFloat","toFixed","padEnd","formatBytes","bytes","decimals","dm","floor","pow","getNanosecondTimeViaPerformance","performance","formatNanoseconds","getNanosecondsTime","WorkerGlobalScope","self","process","release","name","hrtime","bigint","uniqueId","getOwnProperty","object","property","hasOwn","hasOwnProperty","call","sortTokenScorePredicate","intersect","tmp","elem","found","count","filter","e","getDocumentProperties","doc","paths","properties","pathsLength","path","pathTokens","pathTokensLength","lat","lon","isArray","getNested","obj","mapDistanceToMeters","cm","m","km","ft","yd","mi","convertDistanceToMeters","unit","ratio","message","removeVectorsFromHits","searchResult","vectorProperties","hits","result","document","reduce","acc","prop","lastKey","pop","key"],"sourceRoot":""}